# -*- coding: utf-8 -*-
"""KV_final_novo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EFCWhox6DHE6SnGapBNw6wY5SscfBWzU
"""

!pip install --upgrade pip

from tensorflow.python.client import device_lib
print(device_lib.list_local_devices())

!pip install focal_loss

from google.colab import drive
drive.mount('/content/drive/')
drive_path = '/content/drive/MyDrive/MyModels/'

!unzip '/content/drive/My Drive/Dataset/MURA-v1.1.zip'

import os
import numpy as np
import cv2
import random
import keras.backend as K
import matplotlib.pyplot as plt
import keras
from keras.models import Model
from keras.layers import Input, Dense, Conv2D
from keras.layers import Flatten, MaxPool2D, BatchNormalization
from keras.layers import Concatenate, Add, Dropout, ReLU, Lambda, Activation, LeakyReLU
from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, Callback
from keras.preprocessing.image import ImageDataGenerator
from keras import regularizers
from keras.applications.densenet import DenseNet169
from keras.layers import MaxPooling2D, GlobalAveragePooling2D

#%% Ucitavanje podataka
def traverse_directory(directory):
    images_lst = []
    labels_lst = []
    for root, patient_dirs, files in os.walk(directory):
        print(root)
        for patient_dir in patient_dirs:
            patient_directory = os.path.join(root, patient_dir)
            print(patient_directory)
            for patient_root, study_dirs, patient_files in os.walk(patient_directory):
                for study_dir in study_dirs:
                    study_directory = os.path.join(patient_root, study_dir)
                    print(study_directory)
                    for study_root, file_dirs, study_files in os.walk(study_directory):
                        for file in study_files:
                            file_path = os.path.join(study_root, file)
                            print(file_path)  # Do something with the file
                            curr_image = cv2.imread(file_path)
                            if curr_image is not None:
                              images_lst.append(curr_image)
                            if ('negative' in file_path):
                                labels_lst.append(0)
                            elif ('positive' in file_path):
                                labels_lst.append(1)

    return images_lst, labels_lst
#%%

def read_data(path_dataset, type_dataset, body_part):
    full_path = path_dataset + '/' + type_dataset + '/' + body_part
    return traverse_directory(full_path)

#%%
def plot_images(images_lst, labels_lst, body_part, dataset_type):
    random_ind = random.sample(range(len(images_lst)), 4)

    fig,ax = plt.subplots(2,2)

    plt.figure()
    ax = ax.ravel()
    for k in range(4):
        label_ind = labels_lst[random_ind[k]]
        if (label_ind == 0):
            label_str = 'Normal'
        elif (label_ind == 1):
            label_str = 'Abnormal'
        ax[k].imshow(images_lst[random_ind[k]])
        ax[k].axis('off')
        ax[k].set_title('Primer slike - ' + label_str)

    fig.suptitle('Skup: ' + dataset_type + ', deo tela: ' + body_part)
    plt.show()


#%% Statistika o podacima

def data_statistics(images, labels, type_dataset, body_part):
    # Ukupan broj podataka u skupu train/val
    num_samples = (len(images))
    # Ukupan broj pozitiva u jednom skupu (train/val)
    num_pos_samples = sum(labels)
    # Ukupan broj negativa u jednom skupu (train/val)
    num_neg_samples = num_samples - num_pos_samples

    image_height = []
    image_width = []
    for image in images:
      image_height.append(image.shape[0])
      image_width.append(image.shape[1])

    plt.figure()
    plt.hist(image_height, color = 'black')
    plt.title('Histogram velicine slika: visina, skup: ' + type_dataset +
              ", deo tela: " + body_part)
    plt.xlabel('Broj piksela')

    plt.figure()
    plt.hist(image_width, color = 'black')
    plt.title('Histogram velicine slika: sirina, skup: ' + type_dataset +
              ", deo tela: " + body_part)
    plt.xlabel('Broj piksela')

    print("O podacima - deo tela: " + body_part +
          "\nUkupan broj podataka, skup: " + type_dataset + " - "
          + str(num_samples) + "\nBroj pozitiva  - " + str(num_pos_samples)
          + "\nBroj negativa - " + str(num_neg_samples) +
          "\nUdeo pozitiva u skupu podataka - " +
          str(100*num_pos_samples/num_samples) + "%" +
          "\nUdeo negativa u skupu podataka - " +
          str(100*num_neg_samples/num_samples) + "%")
    return num_samples, num_pos_samples, num_neg_samples

#%% Pretprocesiranje

def get_train_generator(images_lst, labels_lst, b_size):
    datagen = ImageDataGenerator(
    rotation_range=30,
    horizontal_flip=True
    )

    datagen.fit(images_lst,augment=True)

    image_generator = datagen.flow(
        x = images_lst,
        y = labels_lst,
        batch_size = b_size
    )

    return image_generator


def get_valid_generator(images_lst, labels_lst, b_size, std):
    datagen = ImageDataGenerator(
    rescale = 1/std
    )

    datagen.fit(images_lst,augment=False)

    image_generator = datagen.flow(
        x = images_lst,
        y = labels_lst,
        batch_size = b_size
    )

    return image_generator


#%% Model - DesNet169
def denseNet169_model(size):

    base_model = DenseNet169(input_shape=(None,None,3),
                             weights='imagenet',
                             include_top=False,
                             pooling='avg')


    x = base_model.output

    predictions = Dense(1,activation='sigmoid')(x)
    model = Model(inputs=base_model.input, outputs=predictions)
    return model

def custom_model(img_height,img_width):
  #stem
  input = Input(shape=(img_height,img_width,3))
  x = Conv2D(32, (3, 3), padding='valid')(input)
  x = BatchNormalization()(x)
  x = Conv2D(64, (3, 3), padding='valid')(x)
  x = BatchNormalization()(x)
  x = Conv2D(64, (3, 3), padding='valid')(x)
  x = BatchNormalization()(x)
  x = MaxPooling2D((3, 3), strides=(2, 2))(x)

  x = Conv2D(64, (3, 3), padding='valid')(x)
  x = BatchNormalization()(x)
  x = Conv2D(128, (3, 3), padding='valid')(x)
  x = BatchNormalization()(x)
  x = Conv2D(128, (3, 3), padding='valid')(x)
  x = BatchNormalization()(x)
  x = MaxPooling2D((3, 3), strides=(2, 2))(x)

  x = Conv2D(128, (3, 3), padding='valid')(x)
  x = BatchNormalization()(x)
  x = Conv2D(256, (3, 3), padding='valid')(x)
  x = BatchNormalization()(x)
  x = Conv2D(256, (3, 3), padding='valid')(x)
  x = BatchNormalization()(x)
  x = MaxPooling2D((3, 3), strides=(2, 2))(x)


  x = GlobalAveragePooling2D()(x)
  x = Dropout(0.1)(x)
  predictions = Dense(1,activation='sigmoid',name="final")(x) #NOTE: here, not applying l2 reg.
  model = Model(inputs= input, outputs=predictions)
  return model

#%%
path_dataset = 'MURA-v1.1'
type_dataset = 'train'
#body_part = 'XR_HUMERUS'
body_part = 'XR_FOREARM'

images_train, lables_train = read_data(path_dataset, type_dataset, body_part)

len(images_train)

#%% primer slike
#plot_images(images_train, lables_train, 'HUMERUS', 'Obucavajuci')
plot_images(images_train, lables_train , 'FOREARM', 'Obucavajuci')


#%% statistika
#num_samples, num_pos_samples, num_neg_samples = data_statistics(images_train,
#                                                                lables_train,
#                                                                'Obucavajuci',
#                                                                'HUMERUS')

num_samples, num_pos_samples, num_neg_samples = data_statistics(images_train,
                                                                lables_train,
                                                                'Obucavajuci',
                                                                'FOREARM')

#%%
from sklearn.utils import class_weight
# Calculate the weights for each class so that we can balance the data
weights = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(lables_train), y=lables_train)

#%% preprocesiranje
size = 320
b_size = 32
for k in range(len(images_train)):
   images_train[k] = cv2.resize(images_train[k],(size,size))

images_train = np.array(images_train).astype('float32')
mean_train = np.mean(images_train)
std_train = np.std(images_train)
images_train = (images_train - mean_train)/std_train
image_train_generator = get_train_generator(images_train, lables_train, b_size)

#%% VALIDACIONI
path_dataset = 'MURA-v1.1'
type_dataset = 'valid'
#body_part = 'XR_HUMERUS'
body_part = 'XR_FOREARM'
images_valid, labels_valid = read_data(path_dataset, type_dataset, body_part)

from sklearn.model_selection import train_test_split

random.seed(42)

good_indexes = [i for i, lbl in enumerate(labels_valid) if lbl == 1]
bad_indexes = [i for i, lbl in enumerate(labels_valid) if lbl == 0]

random.shuffle(good_indexes)
random.shuffle(bad_indexes)

N_good = len(good_indexes)//3
N_bad = len(bad_indexes)//3

# Split the list
images_test = [images_valid[i] for i in good_indexes[:N_good+1]] + [images_valid[i] for i in bad_indexes[:N_bad+1]]
images_valid = [images_valid[i] for i in good_indexes[N_good+1:]] + [images_valid[i] for i in bad_indexes[N_bad+1:]]
labels_test = [labels_valid[i] for i in good_indexes[:N_good+1]] + [labels_valid[i] for i in bad_indexes[:N_bad+1]]
labels_valid = [labels_valid[i] for i in good_indexes[N_good+1:]] + [labels_valid[i] for i in bad_indexes[N_bad+1:]]

sum(labels_valid)

#%% primer slike
#plot_images(images_valid, labels_valid, 'HUMERUS', 'Validacioni')
plot_images(images_valid, labels_valid, 'FOREARM', 'Validacioni')

#%% statistika
#num_samples_valid, num_pos_samples_valid, num_neg_samples_valid = data_statistics(images_valid,
#                                                                labels_valid,
#                                                                'Validacioni',
#                                                                'HUMERUS')

num_samples_valid, num_pos_samples_valid, num_neg_samples_valid = data_statistics(images_valid,
                                                                labels_valid,
                                                                'Validacioni',
                                                                'FOREARM')

#%% primer slike
#plot_images(images_test,labels_test, 'HUMERUS', 'Testirajuci')
plot_images(images_test,labels_test, 'FOREARM', 'Testirajuci')

#%% statistika
#num_samples_test, num_pos_samples_test, num_neg_samples_test = data_statistics(images_test,
#                                                                labels_test,
#                                                                'Testirajuci',
#                                                                'HUMERUS')

num_samples_test, num_pos_samples_test, num_neg_samples_test = data_statistics(images_test,
                                                                labels_test,
                                                                'Testirajuci',
                                                                'FOREARM')

#%% preprocesiranje validacionog
for k in range(len(images_valid)):
    images_valid[k] = cv2.resize(images_valid[k],(size,size))
images_valid_255 = np.array(images_valid).astype('float32')
images_valid = images_valid_255.copy() - mean_train

for k in range(len(images_test)):
    images_test[k] = cv2.resize(images_test[k],(size,size))
images_test_255 = np.array(images_test).astype('float32')
images_test = (images_test_255.copy() - mean_train)/std_train

validation_generator = get_valid_generator(images_valid, labels_valid, 32, std_train)

#%%
epochs = 20
batch_size = 32

#%%
from keras.applications.densenet import DenseNet169
#model = denseNet169_model(size)
model = custom_model(size,size)
from keras.losses import BinaryCrossentropy
from focal_loss import BinaryFocalLoss
from keras.metrics import Accuracy, Precision, Recall, MSE
model.summary()

#%%
model.compile(loss=BinaryCrossentropy(), optimizer='adam',
              metrics=[Precision(), Recall()])

class_weight={0:weights[0], 1:weights[1]}
#%%
early_stop = EarlyStopping(monitor='val_loss', patience=8, verbose=1, min_delta=1e-4)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=1, verbose=1, min_lr=0.0001)
callbacks_list = [early_stop, reduce_lr]

images_valid.shape

#%%
#train the module
model_history = model.fit(
    image_train_generator,
    workers=0,
    use_multiprocessing=False,
    epochs=epochs,
    steps_per_epoch = num_samples//batch_size,
    validation_data=validation_generator,
    validation_steps=num_samples_valid //batch_size,
    class_weight = class_weight,
    callbacks=callbacks_list
)

#%%
#model.save(drive_path+"MURA-DenseNet-v1.6_"+body_part+".h5")# PROMENI IME ZA INCEPTION
model.save(drive_path+"MURA-Inception-v1.6_"+body_part+".h5")
import pickle
# with open(drive_path+'model_history_densenet_' + body_part + '.pkl', 'wb') as file:
#     pickle.dump(model_history, file)
with open(drive_path+'model_history_inception_' + body_part + '.pkl', 'wb') as file:
   pickle.dump(model_history, file)

#model = keras.saving.load_model(drive_path+"MURA-DenseNet-v1.6_"+body_part+".h5")
model = keras.saving.load_model(drive_path+"MURA-Inception-v1.6_"+body_part+".h5")

import pickle
body_part = 'XR_FOREARM'
with open(drive_path+'model_history_densenet_' + body_part + '.pkl', 'rb') as file:
  initial_history = pickle.load(file)

std_train

def training_plots(model_history, model_type, body_part):
    plt.figure(0)
    plt.plot(model_history.history['precision_1'],'r')
    plt.plot(model_history.history['val_precision_1'],'k')
    plt.xticks(np.arange(0, 20, 1))
    plt.rcParams['figure.figsize'] = (8, 6)
    plt.xlabel("Epoha")
    plt.ylabel("Preciznost")
    plt.title("Promena preciznosti kroz epohe (" + model_type + ") - " + body_part)
    plt.legend(['Obucavajuci','Validacioni'])
    plt.grid()

    plt.figure(1)
    plt.plot(model_history.history['recall_1'],'r')
    plt.plot(model_history.history['val_recall_1'],'k')
    plt.xticks(np.arange(0, 20, 1))
    plt.rcParams['figure.figsize'] = (8, 6)
    plt.xlabel("Epoha")
    plt.ylabel("Senzitivnost")
    plt.title("Promena senzitivnosti kroz epohe (" + model_type + ") - " + body_part)
    plt.legend(['Obucavajuci','Validacioni'])
    plt.grid()

    plt.figure(2)
    plt.plot(model_history.history['loss'],'r')
    plt.plot(model_history.history['val_loss'],'k')
    plt.xticks(np.arange(0, 20, 1))
    plt.rcParams['figure.figsize'] = (8, 6)
    plt.xlabel("Epoha")
    plt.ylabel("Gubitak")
    plt.title("Promena gubitka kroz epohe (" + model_type + ") - " + body_part)
    plt.legend(['Obucavajuci','Validacioni'])
    plt.grid()

    plt.show()

training_plots(initial_history, 'DenseNet', body_part)
#training_plots(model_history, 'Custom', body_part)

from sklearn.metrics import confusion_matrix, auc, accuracy_score, roc_curve, ConfusionMatrixDisplay
import seaborn as sns
def evaluate_results(images,labels, model_type, body_part, dataset_type):
    predictions = model.predict(images)
    pred_labels = (predictions>0.5).astype('int')

    cm = confusion_matrix(labels, pred_labels)
    cmDisplay = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Normal','Abnormal'])
    plt.figure(figsize = (30,20))
    cmDisplay.plot()
    plt.title('Tacnost modela (' + model_type + '): ' + str(accuracy_score(pred_labels, labels)) +
              ', deo tela: ' + body_part + ', skup: ' + dataset_type)
    plt.show()

    print("Tacnost:", accuracy_score(pred_labels, labels))

    fpr_keras, tpr_keras, thresholds_keras = roc_curve(labels, pred_labels)
    auc_keras = auc(fpr_keras, tpr_keras)
    plt.figure(figsize = (30,20))
    plt.plot([0, 1], [0, 1], 'r--')
    plt.plot(fpr_keras, tpr_keras, 'k', label='Keras (area = {:.3f})'.format(auc_keras))
    plt.xlabel('Stopa laznih pozitiva')
    plt.ylabel('Stopa pravih pozitiva')
    plt.grid()
    plt.title('ROC kriva, model: ' + model_type + ', deo tela: ' + body_part + ', skup: ' + dataset_type)
    plt.legend(loc='best')
    plt.show()


print("Prikaz rezultata nad testirajucem skupu:")
evaluate_results(np.array(images_test),np.array(labels_test), 'DenseNet', body_part, 'testirajuci')
#evaluate_results(np.array(images_test),labels_test, 'Custom', body_part, 'testirajuci')


print("Prikaz rezultata nad validacionom skupu:")
evaluate_results(np.array(images_valid)/std_train,np.array(labels_valid), 'DenseNet', body_part, 'validacioni')
#evaluate_results(np.array(images_valid),labels_valid, 'Custom', body_part, 'validacioni')


